version: "3.8"

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.0
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - default

  kafka:
    image: confluentinc/cp-kafka:7.6.0
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    networks:
      - default

  mysql:
    image: mysql:8.0
    container_name: mysql
    ports:
      - "3306:3306"
    environment:
      MYSQL_ROOT_PASSWORD: password
      MYSQL_DATABASE: olympic_dataset
    volumes:
      - mysql_data:/var/lib/mysql
    command: --default-authentication-plugin=mysql_native_password
    networks:
      - default

  spark-master:
    image: bde2020/spark-master:3.3.0-hadoop3.3
    container_name: spark-master
    environment:
      - SPARK_MODE=master
    ports:
      - "7077:7077"
      - "8083:8080"
    volumes:
      - ./jars:/opt/spark-jars
      - ./dags:/opt/airflow/dags
      - ./spark_data:/tmp/spark_data

    networks:
      - default

  spark-worker-1:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker-1
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
    ports:
      - "8081:8081"
    volumes:
      - ./jars:/opt/spark-jars
      - ./dags:/opt/airflow/dags
      - ./spark_data:/tmp/spark_data

    networks:
      - default

  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: airflow-webserver
    user: root
    restart: always
    depends_on:
      - airflow-scheduler
      - mysql
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: mysql+mysqlconnector://root:password@mysql:3306/olympic_dataset
      AIRFLOW__CORE__FERNET_KEY: pgJver1lVCInLo3gWc9Dtz6J7IeQ3FDijW44scNdDWU=
      AIRFLOW__WEBSERVER__SECRET_KEY: test_web_key
      AIRFLOW__CORE__LOAD_EXAMPLES: False
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: False

    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./spark_data:/tmp/spark_data

    ports:
      - "8082:8080"
    networks:
      - default
    command: bash -c "sleep 20 && airflow webserver"
  airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: airflow-scheduler
    user: root
    restart: always
    depends_on:
      - mysql
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: mysql+mysqlconnector://root:password@mysql:3306/olympic_dataset
      AIRFLOW__CORE__FERNET_KEY: pgJver1lVCInLo3gWc9Dtz6J7IeQ3FDijW44scNdDWU=
      AIRFLOW__WEBSERVER__SECRET_KEY: test_web_key
      AIRFLOW__LOGGING__REMOTE_LOGGING: False
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./spark_data:/tmp/spark_data

    command: bash -c "sleep 20 && airflow scheduler"
    networks:
      - default

volumes:
  mysql_data:

networks:
  default:
    driver: bridge
