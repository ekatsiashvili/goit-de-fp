FROM apache/airflow:2.2.5-python3.7

USER root

# Оновлюємо джерела Debian, щоб уникнути помилки NO_PUBKEY
RUN apt-get update && \
    apt-get install -y curl gnupg ca-certificates && \
    echo "deb http://deb.debian.org/debian oldstable main contrib non-free" > /etc/apt/sources.list && \
    curl -fsSL https://ftp-master.debian.org/keys/archive-key-11.asc | gpg --dearmor -o /etc/apt/trusted.gpg.d/debian-archive.gpg && \
    apt-get update

# Встановлюємо Java 11
RUN apt-get update && apt-get install -y openjdk-11-jdk && apt-get clean

# Змінна JAVA_HOME
ENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
ENV PATH="$JAVA_HOME/bin:$PATH"

# Встановлюємо Spark
ENV SPARK_VERSION=3.3.0
ENV HADOOP_VERSION=3
ENV SPARK_HOME=/opt/spark

RUN curl -fsSL "https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz" \
    | tar -xz -C /opt/ && \
    mv /opt/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} ${SPARK_HOME} && \
    ln -s ${SPARK_HOME}/bin/spark-submit /usr/bin/spark-submit && \
    ln -s ${SPARK_HOME}/bin/spark-shell /usr/bin/spark-shell

# Створення директорії для Spark-даних
RUN mkdir -p /tmp/spark_data/landing && chmod -R 777 /tmp/spark_data

# Повертаємо користувача
USER airflow

# Встановлення Python-пакетів
RUN pip install --no-cache-dir \
    apache-airflow-providers-apache-spark \
    apache-airflow-providers-mysql \
    mysql-connector-python \
    pyspark==3.3.0 \
    requests

HEALTHCHECK CMD curl --fail http://localhost:8080/health || exit 1
